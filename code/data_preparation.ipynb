{"cells": [{"cell_type": "markdown", "metadata": {}, "source": "# Data preparation notebook using BigQuery in GCP "}, {"cell_type": "markdown", "metadata": {}, "source": "#### NB:\nthere are two parts in this notebook, the first one create tables in bigquery database to use them later in processing stages on Google cloud platforme, the second part creates mtx files for each dataset ( the adjacency matrix of each network ) to use eather on GCP or in local machine"}, {"cell_type": "code", "execution_count": 5, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Scala code runner version 2.12.10 -- Copyright 2002-2019, LAMP/EPFL and Lightbend, Inc.\r\n"}], "source": "!scala -version"}, {"cell_type": "code", "execution_count": 19, "metadata": {}, "outputs": [], "source": "from scipy.sparse import csr_matrix\nimport numpy as np \nfrom google.cloud import storage\nfrom google.cloud import bigquery\nfrom pyspark.sql import *\nimport pandas as pd\nfrom scipy.io import mmwrite\n\nspark = SparkSession.builder \\\n  .appName('data-preparation')\\\n  .config('spark.jars', 'gs://spark-lib/bigquery/spark-bigquery-latest_2.12.jar') \\\n  .getOrCreate()"}, {"cell_type": "markdown", "metadata": {}, "source": "### Part one: BigQuery tables creation "}, {"cell_type": "code", "execution_count": 12, "metadata": {}, "outputs": [], "source": "files = ['CA-AstroPh', 'CA-GrQc',  'CA-HepPh', 'CA-HepTh' ]\n# the bucket where we store the edges of each dataset in cloud storage \nBUCKET_NAME = 'rplace-bucket'\n# project name in GCP \nproject_name = \"bigdata-project-346922\"\ndataset = \"collaboration_data\"\n\nclient = storage.Client()\nbucket = client.get_bucket(BUCKET_NAME)\n# try to bring data to current repository from cloud storage\nfor file in files :\n    blob = bucket.get_blob('data/' + file + '.txt')\n    blob.download_to_filename(file + '.txt')\n    "}, {"cell_type": "code", "execution_count": 14, "metadata": {}, "outputs": [], "source": "edge = Row('fromId', 'toId')\nclient = bigquery.Client()\njob_config = bigquery.LoadJobConfig(\n    schema=[\n        bigquery.SchemaField(\"fromId\", bigquery.enums.SqlTypeNames.INTEGER),\n        bigquery.SchemaField(\"toId\", bigquery.enums.SqlTypeNames.INTEGER),\n    ],\n    write_disposition=\"WRITE_TRUNCATE\",\n)"}, {"cell_type": "code", "execution_count": 17, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Reading the CA-AstroPh dataset file\nReading the CA-GrQc dataset file\nReading the CA-HepPh dataset file\nReading the CA-HepTh dataset file\n"}], "source": "# iterating over all files to create foreach dataset an edges table \nfor file in files : \n    edges = []\n    indexTable = []\n    index = 1\n    with open(file + '.txt', 'r') as f:\n        print(\"Reading the {0} dataset file\".format(file))\n        content = f.readlines()\n        for line in content[4:]:\n            i = int(line.rsplit()[0])\n            j = int(line.rsplit()[1])\n            edges.append(edge(i,j))\n            indexTable.append(index)\n            index += 1 \n    table_id = project_name + '.' + dataset + '.' + file\n    df = pandas.DataFrame(\n            edges,\n            columns=[\n                \"fromId\",\n                \"toId\"\n                ],\n            index=pandas.Index(\n            indexTable, name=\"id\"\n            ),\n        )\n    job= client.load_table_from_dataframe(\n        df, table_id, job_config=job_config\n    ) "}, {"cell_type": "markdown", "metadata": {}, "source": "#### verification step :\nforeach created table we do request to bigquery API to retrieve tables ! "}, {"cell_type": "code", "execution_count": 18, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Loaded 396160 rows and 3 columns to bigdata-project-346922.collaboration_data.CA-AstroPh\nLoaded 28980 rows and 3 columns to bigdata-project-346922.collaboration_data.CA-GrQc\nLoaded 237010 rows and 3 columns to bigdata-project-346922.collaboration_data.CA-HepPh\nLoaded 51971 rows and 3 columns to bigdata-project-346922.collaboration_data.CA-HepTh\n"}], "source": "# we assert that tables are created and data is loaded !\nfor file in files: \n    table_id = project_name + '.' + dataset + '.' + file\n    table = client.get_table(table_id) \n    print(\n            \"Loaded {} rows and {} columns to {}\".format(\n            table.num_rows, len(table.schema), table_id\n            )\n        )"}, {"cell_type": "markdown", "metadata": {}, "source": "### Part Two: creating sparse matrix for use without bigquery \n#### creation step :\nforeach file, we create an adjacency matrix in sparse format ( for storage constraints ! ) "}, {"cell_type": "code", "execution_count": 20, "metadata": {}, "outputs": [], "source": "for file in files:\n    data = []\n    cols = []\n    rows = []\n    node = 0\n    print(\"Creating sparse matrix for {0} dataset\".format(file))\n    # reading files content in list of lines \n    with open(file + '.txt') as f:\n        content = f.readlines()\n    # lines 0 - 3 are not included in edges, extra informations ...\n    for line in content[4:]:\n        i = int(line.rsplit()[0])\n        j = int(line.rsplit()[1])\n        cols.append(j-1)\n        rows.append(i-1)\n        data.append(1)\n    # identifiersList are index list for each node value\n    # this step is for reducing the size of adj matrix \n    # \n    identifiersList = [i for i in set(cols + rows)]\n    identifiersList.sort()\n    dic = dict()\n    node = 0\n    for i in identifiersList:\n        dic[i] = node\n        node+=1\n    for k in range(len(cols)) : \n        cols[k] = dic[cols[k]]\n        rows[k] = dic[rows[k]]\n    bgId = max(set(cols + rows))\n    m = csr_matrix((data, (rows, cols)), shape=(bgId+1, bgId+1), dtype=np.uint16)\n    mmwrite(file + '.mtx',m)"}, {"cell_type": "markdown", "metadata": {}, "source": "#### verification step : \nwe show created files in last step"}, {"cell_type": "code", "execution_count": 24, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "./CA-AstroPh.mtx  ./CA-GrQc.mtx  ./CA-HepPh.mtx  ./CA-HepTh.mtx  ./email.mtx\r\n"}], "source": "# verification of the cration of files \n! ls ./*.mtx"}, {"cell_type": "markdown", "metadata": {}, "source": "#### optional step \nHere, we are trying to save .mtx files of each dataset to our working bucket in cloud storage "}, {"cell_type": "code", "execution_count": 28, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Copying file://./CA-AstroPh.mtx [Content-Type=application/octet-stream]...\nCopying file://./CA-GrQc.mtx [Content-Type=application/octet-stream]...         \nCopying file://./CA-HepPh.mtx [Content-Type=application/octet-stream]...        \nCopying file://./CA-HepTh.mtx [Content-Type=application/octet-stream]...        \nCopying file://./email.mtx [Content-Type=application/octet-stream]...           \n/ [5/5 files][  4.3 MiB/  4.3 MiB] 100% Done                                    \nOperation completed over 5 objects/4.3 MiB.                                      \n"}], "source": "!gsutil -m cp ./*.mtx gs://rplace-bucket/data"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.4"}}, "nbformat": 4, "nbformat_minor": 2}